{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d59560a6-d63b-4fa0-83e5-53ee411acedf",
   "metadata": {},
   "source": [
    "# Exercise 5: Variance reduction methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "id": "32d4873d-9cde-4d0c-ae45-0ef71aee130f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import scipy.stats as st "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "id": "ffdab600-6cbf-4348-a9c8-2bbd51a0c16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100 # sample size\n",
    "exact_solution = math.e - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a45db0-02dc-47ce-a426-1252885d3b2b",
   "metadata": {},
   "source": [
    "## 1.\n",
    "Estimate the integral $$\\int_0^1 e^x dx$$ by simulation (the crude Monte Carlo estimator). Use e.g. an estimator based on 100 samples and present the result as the point estimator and a confidence interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "id": "a8b6b4ec-09c1-49c5-acdb-6bf24bfc55a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact solution: 1.718281828459045\n",
      "The crude Monte Carlo point estimator: 1.6934037632538963\n",
      "The variance: 0.2509606866355765\n",
      "95% confidence interval: (1.5940024773838246, 1.792805049123968)\n"
     ]
    }
   ],
   "source": [
    "def crude_MC(n):\n",
    "    U = np.random.uniform(0,1,size=n) # 100 random samples of U(0,1)\n",
    "    X = []\n",
    "\n",
    "    for i in range(n):\n",
    "        X_i = np.exp(U[i])\n",
    "        X.append(X_i)\n",
    "    return X\n",
    "\n",
    "X = crude_MC(n)\n",
    "mean = np.mean(X)\n",
    "std = np.std(X, ddof=1)\n",
    "\n",
    "# Calculate the 95% confidence interval\n",
    "ci = st.t.interval(0.95, df=n-1, loc=mean, scale=std / np.sqrt(n))\n",
    "\n",
    "print(f\"Exact solution: {exact_solution}\")\n",
    "print(f\"The crude Monte Carlo point estimator: {mean}\")\n",
    "print(f\"The variance: {std**2}\")\n",
    "print(f\"95% confidence interval: {ci}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35d00d6-e3d8-4851-adfc-30bbfe78f8d7",
   "metadata": {},
   "source": [
    "## 2.\n",
    "Estimate the integral $$\\int_0^1 e^x dx$$ using antithetic variables, with comparable computer ressources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "id": "d9665e53-6b21-4336-a025-43705260183a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact solution: 1.718281828459045\n",
      "The Antithetic point estimator: 1.7173343023652092\n",
      "The variance: 0.004122805031632862\n",
      "95% confidence interval: (1.7045938292933909, 1.7300747754370276)\n"
     ]
    }
   ],
   "source": [
    "def antithetic(n):\n",
    "    U = np.random.uniform(0,1,size=n) # 100 random samples of U(0,1)\n",
    "    Y = []\n",
    "\n",
    "    for i in range(n):\n",
    "        Y_i = (np.exp(U[i]) + np.exp(1-U[i]))/2\n",
    "        Y.append(Y_i)\n",
    "    return Y\n",
    "\n",
    "Y = antithetic(n)\n",
    "mean = np.mean(Y)\n",
    "std = np.std(Y, ddof=1)\n",
    "\n",
    "# Calculate the 95% confidence interval\n",
    "ci = st.t.interval(0.95, df=n-1, loc=mean, scale=std / np.sqrt(n))\n",
    "\n",
    "print(f\"Exact solution: {exact_solution}\")\n",
    "print(f\"The Antithetic point estimator: {mean}\")\n",
    "print(f\"The variance: {std**2}\")\n",
    "print(f\"95% confidence interval: {ci}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb962f1-5000-4bb0-b5b7-64a89e444409",
   "metadata": {},
   "source": [
    "## 3. \n",
    "Estimate the integral $$\\int_0^1 e^x dx$$ using a control variable, with comparable computer ressources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "id": "4c955a4a-48a8-47e7-a5a4-269f55fad42e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact solution: 1.718281828459045\n",
      "The Control variable point estimator: 1.713973517500537\n",
      "The variance: 0.003435586783460785\n",
      "95% confidence interval: (1.7023432523173248, 1.7256037826837494)\n"
     ]
    }
   ],
   "source": [
    "def control(n):\n",
    "    U = np.random.uniform(0,1,size=n) # 100 random samples of U(0,1)\n",
    "    Z = []\n",
    "    \n",
    "    X = np.exp(U)\n",
    "    Y = U\n",
    "    c = -np.cov(X,Y)[0, 1]/np.var(Y)\n",
    "    for i in range(n):\n",
    "        Z_i = X[i] + c*(U[i]-1/2) # X_i = exp(U_i)\n",
    "        Z.append(Z_i)\n",
    "    return Z\n",
    "\n",
    "Z = control(n)\n",
    "mean = np.mean(Z)\n",
    "std = np.std(Z, ddof=1)\n",
    "\n",
    "# Calculate the 95% confidence interval\n",
    "ci = st.t.interval(0.95, df=n-1, loc=mean, scale=std / np.sqrt(n))\n",
    "\n",
    "print(f\"Exact solution: {exact_solution}\")\n",
    "print(f\"The Control variable point estimator: {mean}\")\n",
    "print(f\"The variance: {std**2}\")\n",
    "print(f\"95% confidence interval: {ci}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88717c6-dfff-48e2-aeec-e9d52f2423d9",
   "metadata": {},
   "source": [
    "## 4. \n",
    "Estimate the integral $$\\int_0^1 e^x dx$$ using stratified sampling, with comparable computer ressources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "id": "34ea5ae9-3db7-4768-b832-b400c55052c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact solution: 1.718281828459045\n",
      "The Control variable point estimator: 1.7212239480694496\n",
      "The variance: 0.00031054285400649385\n",
      "95% confidence interval: (1.7177273160871103, 1.7247205800517889)\n"
     ]
    }
   ],
   "source": [
    "def stratified(n, m):\n",
    "    W = []\n",
    "    U = np.random.uniform(0,1,size=(n,m))\n",
    "    for i in range(n):\n",
    "        w_i = 0\n",
    "        for j in range(m):\n",
    "            w_i += np.exp(j/m+U[i,j]/m)\n",
    "        W.append(w_i/m)\n",
    "    return W\n",
    "\n",
    "strata = 10\n",
    "W = stratified(n,strata)\n",
    "mean = np.mean(W)\n",
    "std = np.std(W, ddof=1)\n",
    "\n",
    "# Calculate the 95% confidence interval\n",
    "ci = st.t.interval(0.95, df=n-1, loc=mean, scale=std / np.sqrt(n))\n",
    "\n",
    "print(f\"Exact solution: {exact_solution}\")\n",
    "print(f\"The Control variable point estimator: {mean}\")\n",
    "print(f\"The variance: {std**2}\")\n",
    "print(f\"95% confidence interval: {ci}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d83dda-3e82-49d0-b2df-41eff3b26363",
   "metadata": {},
   "source": [
    "## 5.\n",
    "Use control variates to reduce the variance of the estimator in exercise 4 (Poisson arrivals)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "id": "7f9a9237-2042-45db-8f5d-38d6abcbfd5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean fraction of blocked customers: 0.12060231843737668\n",
      "Variance of fraction of blocked customers: 6.414020317816754e-06\n",
      "95% confidence interval: (0.11879061153146984, 0.12241402534328351)\n"
     ]
    }
   ],
   "source": [
    "# Known Parameters:\n",
    "m = 10 # number of service units\n",
    "mst = 8 # mean service time, in time units\n",
    "mtbc = 1 # mean time between customers, in time units\n",
    "batches = 10 # number of batches\n",
    "customers_per_batch = 10_000 # number of customers per batch\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import poisson, norm\n",
    "import scipy.stats as stats\n",
    "\n",
    "# arrival times are poisson processes\n",
    "# service times are exponential\n",
    "def sim():\n",
    "    at = np.cumsum(np.random.exponential(scale=mtbc, size=customers_per_batch)) # Poisson arrival times: CDF = P(Xi ≤ t) = 1 − e−λt\n",
    "    st = np.random.exponential(scale=mst, size=customers_per_batch) # Exponential service times\n",
    "\n",
    "    # Event queue for busy service units m\n",
    "    service_busy_until = np.zeros(m)\n",
    "    blocked_customers = 0\n",
    "\n",
    "    # Iterate through each customer\n",
    "    for i in range(customers_per_batch):\n",
    "        \n",
    "        # Check if any service unit is free\n",
    "        if any(service_busy_until <= at[i]):\n",
    "            \n",
    "            # Find the first free service unit\n",
    "            not_in_use = np.where(service_busy_until <= at[i])[0][0]\n",
    "            service_busy_until[not_in_use] = at[i] + st[i]\n",
    "        else:\n",
    "            blocked_customers += 1\n",
    "\n",
    "    return blocked_customers / customers_per_batch, np.mean(np.diff(at))\n",
    "\n",
    "blocked_results = [sim () for batch in range(batches)]\n",
    "\n",
    "X, Y = zip(*blocked_results)\n",
    "\n",
    "# Control variate is the mean of the interarrival times which is 1/lambda = 1/mtbc = 1\n",
    "control_blocked_frac = []\n",
    "c = -np.cov(X, Y)[0, 1] / np.var(Y)\n",
    "for i in range(batches):\n",
    "    Z_i = X[i] + c * (Y[i] - 1 / mtbc)\n",
    "    control_blocked_frac.append(Z_i)\n",
    "\n",
    "# Calculate mean and standard deviation\n",
    "mean_blocked_fraction = np.mean(control_blocked_frac)\n",
    "std_blocked_fraction = np.std(control_blocked_frac, ddof=1)\n",
    "\n",
    "# Calculate the 95% confidence interval\n",
    "ci = stats.t.interval(0.95, df=batches-1, loc=mean_blocked_fraction, scale=std_blocked_fraction / np.sqrt(batches))\n",
    "\n",
    "print(f\"Mean fraction of blocked customers: {mean_blocked_fraction}\")\n",
    "print(f\"Variance of fraction of blocked customers: {std_blocked_fraction**2}\")\n",
    "print(f\"95% confidence interval: {ci}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c520ca88-b5d2-4c79-8d75-9157a8caf76b",
   "metadata": {},
   "source": [
    "## 6.\n",
    "Demonstrate the effect of using common random numbers in exercise 4 for the difference between Poisson arrivals (Part 1) and a renewal process with hyperexponential interarrival times. **Remark:** You might need to do some thinking and some re-programming.\n",
    "\n",
    "**Solution:**\n",
    "\n",
    "Let's apply inverse transform sampling to generate samples from an exponential distribution with rate parameter $\\lambda$.\n",
    "1. Exponential CDF\n",
    "\n",
    "    For an exponential distribution with rate $\\lambda$ :\n",
    "    $$\n",
    "    f_X(x)=\\lambda e^{-\\lambda x}, \\quad x \\geq 0\n",
    "    $$\n",
    "    \n",
    "    The CDF is:\n",
    "    $$\n",
    "    F_X(x)=1-e^{-\\lambda x}\n",
    "    $$\n",
    "2. Inverse CDF\n",
    "\n",
    "    To find the inverse CDF, we set $F_X(x)=u$ and solve for $x$ :\n",
    "    $$\n",
    "    \\begin{aligned}\n",
    "    & u=1-e^{-\\lambda x} \\\\\n",
    "    & e^{-\\lambda x}=1-u \\\\\n",
    "    & -\\lambda x=\\ln (1-u) \\\\\n",
    "    & x=-\\frac{\\ln (1-u)}{\\lambda}\n",
    "    \\end{aligned}\n",
    "    $$\n",
    "    \n",
    "    Since $U$ is uniformly distributed between 0 and $1,1-U$ is also uniformly distributed between 0 and 1. For simplicity, we can use $U$ directly:\n",
    "    $$\n",
    "    x=-\\frac{\\ln (U)}{\\lambda}.\n",
    "    $$\n",
    "\n",
    "Now we can generate exponential and hyper-exponential inter-arrival times with a uniformly distributed variable $U$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "id": "30b82f6e-8ee0-41c3-bada-1bd4eb2cc8c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance between the difference 0.007495231817629124\n",
      "Variance between the difference using common random variables U: 0.006383298520357635\n"
     ]
    }
   ],
   "source": [
    "# Known Parameters:\n",
    "m = 10 # number of service units\n",
    "mst = 8 # mean service time, in time units\n",
    "mtbc = 1 # mean time between customers, in time units\n",
    "batches = 10 # number of batches\n",
    "customers_per_batch = 10_000 # number of customers per batch\n",
    "p1, lambda1, p2, lambda2 = 0.8, 0.8333, 0.2, 5.0 # Settings for the hyper-exponential distribution\n",
    "\n",
    "def sim(at):\n",
    "    st = np.random.exponential(scale=mst, size=customers_per_batch) # Exponential service times\n",
    "    service_busy_until = np.zeros(m)\n",
    "    blocked_customers = 0\n",
    "\n",
    "    for i in range(customers_per_batch):\n",
    "        if any(service_busy_until <= at[i]):\n",
    "            not_in_use = np.where(service_busy_until <= at[i])[0][0]\n",
    "            service_busy_until[not_in_use] = at[i] + st[i]\n",
    "        else:\n",
    "            blocked_customers += 1\n",
    "\n",
    "    return blocked_customers / customers_per_batch\n",
    "\n",
    "\n",
    "### Not using a common random variable U ###\n",
    "# Not using a common random number\n",
    "at_exp = np.cumsum(np.random.exponential(scale=mtbc, size=customers_per_batch))\n",
    "\n",
    "# Uniform variable only used to generate the hyper-exponential distribution\n",
    "U_hyperexp = np.random.uniform(0, 1, size=customers_per_batch)\n",
    "\n",
    "at_hyperexp = np.cumsum(np.where(\n",
    "                        U_hyperexp < p1, \n",
    "                            np.random.exponential(scale=1/lambda1, size=customers_per_batch), \n",
    "                            np.random.exponential(scale=1/lambda2, size=customers_per_batch)\n",
    "                        ))\n",
    "\n",
    "# Exponential result\n",
    "blocked_frac_poisson = [sim(at_exp) for batch in range(batches)]\n",
    "\n",
    "# Hyper-exponential result with U\n",
    "blocked_frac_hyperexponential = [sim(at_hyperexp) for batch in range(batches)]\n",
    "\n",
    "diff = np.array(blocked_frac_poisson) - np.array(blocked_frac_hyperexponential)\n",
    "\n",
    "# Calculate variance of difference between poisson and hyper-exponential results\n",
    "variance_of_diff = np.std(diff)\n",
    "\n",
    "### Using a common random variable U ###\n",
    "\n",
    "# Generating uniform variables from 0 to 1, that are used in creating the two different arrival times\n",
    "U = np.random.uniform(0, 1, size=customers_per_batch)\n",
    "\n",
    "# Exponential inter-arrival times\n",
    "at_exp_U = np.cumsum(-np.log(U)/mtbc)\n",
    "\n",
    "# Hyper-exponential inter-arrival times\n",
    "at_hyperexp_U = np.cumsum(np.where(\n",
    "                        U < p1, \n",
    "                            np.random.exponential(scale=1/lambda1, size=customers_per_batch), \n",
    "                            np.random.exponential(scale=1/lambda2, size=customers_per_batch)\n",
    "                        ))\n",
    "\n",
    "# Exponential result with U\n",
    "blocked_frac_poisson_U = [sim(at_exp_U) for batch in range(batches)]\n",
    "\n",
    "# Hyper-exponential result with U\n",
    "blocked_frac_hyperexponential_U = [sim(at_hyperexp_U) for batch in range(batches)]\n",
    "\n",
    "diff_U = np.array(blocked_frac_poisson_U) - np.array(blocked_frac_hyperexponential_U)\n",
    "\n",
    "# Calculate variance of difference between poisson and hyper-exponential results\n",
    "variance_of_diff_U = np.std(diff_U)\n",
    "\n",
    "print(f\"Variance between the difference {variance_of_diff}\")\n",
    "print(f\"Variance between the difference using common random variables U: {variance_of_diff_U}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d640220-985f-4111-b69d-c9ba85f219da",
   "metadata": {},
   "source": [
    "## 7. \n",
    "For a standard normal random variable $Z \\sim N(0, 1)$ using the crude Monte Carlo estimator, estimate the probability $Z > a$. Then try importance sampling with a normal density with mean $a$ and variance $\\sigma^2$. For the experiments start using $\\sigma^2$ = 1, use different values of $a$ (e.g. 2 and 4), and different sample sizes. If time permits experiment with other values for $\\sigma^2$. Finally discuss the efficiency of the methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "id": "3680a826-0c83-49e9-9124-2fd134c926b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using a sample size of 1000 and mean a of 2 we get:\n",
      "Estimated probability that Z > 2 using crude Monte Carlo: 0.018\n",
      "Expected probability that Z > 2: 0.02275013194817921\n",
      "\n",
      "Using a sample size of 1000 and mean a of 4 we get:\n",
      "Estimated probability that Z > 4 using crude Monte Carlo: 0.0\n",
      "Expected probability that Z > 4: 3.167124183311998e-05\n",
      "\n",
      "Using a sample size of 10000 and mean a of 2 we get:\n",
      "Estimated probability that Z > 2 using crude Monte Carlo: 0.0236\n",
      "Expected probability that Z > 2: 0.02275013194817921\n",
      "\n",
      "Using a sample size of 10000 and mean a of 4 we get:\n",
      "Estimated probability that Z > 4 using crude Monte Carlo: 0.0001\n",
      "Expected probability that Z > 4: 3.167124183311998e-05\n",
      "\n",
      "Using a sample size of 100000 and mean a of 2 we get:\n",
      "Estimated probability that Z > 2 using crude Monte Carlo: 0.0234\n",
      "Expected probability that Z > 2: 0.02275013194817921\n",
      "\n",
      "Using a sample size of 100000 and mean a of 4 we get:\n",
      "Estimated probability that Z > 4 using crude Monte Carlo: 6e-05\n",
      "Expected probability that Z > 4: 3.167124183311998e-05\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Crude Monte Carlo ##\n",
    "\n",
    "def crude_MC_probability(a, n):\n",
    "    Z = np.random.normal(0,1,size=n)   \n",
    "    return np.sum(Z > a)/n\n",
    "\n",
    "ns = [1_000, 10_000, 100_000]\n",
    "a_s = [2, 4]\n",
    "var = 1\n",
    "for n in ns:\n",
    "    for a in a_s:\n",
    "        p = crude_MC_probability(a, n)\n",
    "        expected_p = 1 - stats.norm.cdf(a, 0, 1)\n",
    "        print(f\"Using a sample size of {n} and mean a of {a} we get:\")\n",
    "        print(f\"Estimated probability that Z > {a} using crude Monte Carlo: {p}\")\n",
    "        print(f\"Expected probability that Z > {a}: {expected_p}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc1a2cc-7f53-4716-bd9f-e5fbabd2ac2a",
   "metadata": {},
   "source": [
    "For the importance sampling we notice that if we look at the structure of the importance sampling\n",
    "$$\\theta=\\int \\frac{h(x) f(x)}{g(x)} g(x) \\mathrm{d} x=\\mathrm{E}\\left(\\frac{h(Y) f(Y)}{g(Y)}\\right)$$\n",
    "that here we choose $f(x)$ as the PDF of a standard normal distribution, $g(x)$ as the PDF of a normal distribution with mean $a$ and variance $\\sigma^2=1$ and $h(x)$ as the function that says if $Z>a$. We then also have $Y$ which is distributed with density $g(y)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "id": "faaf2428-5e6c-46c5-85fd-1d51034a53ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using a sample size of 1000 and mean a of 2 we get:\n",
      "Estimated probability that Z > 2 using crude Monte Carlo: 0.023554125629001534\n",
      "Expected probability that Z > 2: 0.02275013194817921\n",
      "\n",
      "Using a sample size of 1000 and mean a of 4 we get:\n",
      "Estimated probability that Z > 4 using crude Monte Carlo: 3.3153919944585745e-05\n",
      "Expected probability that Z > 4: 3.167124183311998e-05\n",
      "\n",
      "Using a sample size of 10000 and mean a of 2 we get:\n",
      "Estimated probability that Z > 2 using crude Monte Carlo: 0.022516105050181045\n",
      "Expected probability that Z > 2: 0.02275013194817921\n",
      "\n",
      "Using a sample size of 10000 and mean a of 4 we get:\n",
      "Estimated probability that Z > 4 using crude Monte Carlo: 3.160212407140306e-05\n",
      "Expected probability that Z > 4: 3.167124183311998e-05\n",
      "\n",
      "Using a sample size of 100000 and mean a of 2 we get:\n",
      "Estimated probability that Z > 2 using crude Monte Carlo: 0.022868409948656018\n",
      "Expected probability that Z > 2: 0.02275013194817921\n",
      "\n",
      "Using a sample size of 100000 and mean a of 4 we get:\n",
      "Estimated probability that Z > 4 using crude Monte Carlo: 3.16925107890967e-05\n",
      "Expected probability that Z > 4: 3.167124183311998e-05\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Importance sampling ##\n",
    "\n",
    "def importance_sampling(a, sigma, n):\n",
    "    Y = np.random.normal(a, sigma, n)\n",
    "    \n",
    "    f = stats.norm.pdf(Y)\n",
    "    g = stats.norm.pdf(Y, a, sigma)\n",
    "    h = np.where(Y > a, 1, 0)\n",
    "    \n",
    "    return np.mean(h*f/g)\n",
    "\n",
    "ns = [1_000, 10_000, 100_000]\n",
    "a_s = [2, 4]\n",
    "sigma = 1\n",
    "for n in ns:\n",
    "    for a in a_s:\n",
    "        p = importance_sampling(a, sigma, n)\n",
    "        expected_p = 1 - stats.norm.cdf(a, 0, 1)\n",
    "        print(f\"Using a sample size of {n} and mean a of {a} we get:\")\n",
    "        print(f\"Estimated probability that Z > {a} using crude Monte Carlo: {p}\")\n",
    "        print(f\"Expected probability that Z > {a}: {expected_p}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4eb4173-05ee-4955-a733-7eb662e26d50",
   "metadata": {},
   "source": [
    "**Discussion:** We see that the crude Monte Carlo method isn't as effective when we need samples for the extreme cases, as can be easily seen for $a=4$ and $n=1000$ since it outputs 0.0. Here is where the importance sampling comes in as it sort of gives greater weight to these extreme cases and we are able to get these cases more often which generates a better estimate, even with a small sample size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a34dbfa-fa78-45b4-a4f8-4e295566a626",
   "metadata": {},
   "source": [
    "## 8. \n",
    "Use importance sampling with $g(x) = \\lambda \\exp(−\\lambda x)$ to calculate the integral $\\int_0^1 e^x dx$ of Question 1. Try to find the optimal value of $\\lambda$ by calculating the variance of $h(X)f(X)/g(X)$ and verify by simulation. Note that importance sampling with the exponential distribution will not reduce the variance.\n",
    "\n",
    "**Solution:**\n",
    "\n",
    "Here we have $g(x) = \\lambda \\exp(−\\lambda x)$, $h(x) = e^x$ and $f(x)=1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "id": "5fad0f5d-91ae-4fea-beaf-07f6794baf54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal lambda: 1.163265306122449\n",
      "Minimum variance: 2.6310664429416613\n",
      "Estimate of integral: 1.9260175659027263\n"
     ]
    }
   ],
   "source": [
    "def h(x):\n",
    "    return np.exp(x)\n",
    "\n",
    "def importance_sampling_integral(lambda_, n):\n",
    "    Y = np.random.exponential(scale=1/lambda_, size=n)\n",
    "    Y = Y[Y <= 1]  # Truncate to [0, 1]\n",
    "\n",
    "    w = h(Y) / (lambda_ * np.exp(-lambda_ * Y))\n",
    "    \n",
    "    theta = np.mean(w)\n",
    "    var = np.var(w)\n",
    "    \n",
    "    return theta, var\n",
    "\n",
    "n = 10_000\n",
    "# A list of lambda\n",
    "lambdas = np.linspace(1, 5, 50)\n",
    "\n",
    "thetas = []\n",
    "vars = []\n",
    "\n",
    "for lambda_ in lambdas:\n",
    "    theta, var = importance_sampling_integral(lambda_, n)\n",
    "    thetas.append(theta)\n",
    "    vars.append(var)\n",
    "\n",
    "# Find the optimal lambda, that is the one with the minimum variance\n",
    "optimal_lambda = lambdas[np.argmin(vars)]\n",
    "\n",
    "# Print the results\n",
    "print(f\"Optimal lambda: {optimal_lambda}\")\n",
    "print(f\"Minimum variance: {min(vars)}\")\n",
    "\n",
    "print(f\"Estimate of integral: {np.mean(thetas)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b429abec-935f-4f59-aeb0-00dd3ad92e3f",
   "metadata": {},
   "source": [
    "# 9.\n",
    " For the Pareto case derive the IS estimator for the mean using the first moment distribution as sampling distribution. Is the approach meaningful? And could this be done in general? With this insight could you change the choice of $g(x)$ in the previous question (Question 8) such that importance sampling would reduce the variance? You do not need to implement this, as long as you can argue, what should happen.\n",
    "\n",
    "**Solution:** \n",
    "\n",
    "Since we are estimating the mean of a Pareto distribution we have $h(x) = x$, $f(x) = \\alpha \\frac{x_m^{\\alpha}}{x^{\\alpha +1}}$, that is the pareto PDF and finally $g(x)=\\alpha_g \\frac{x_m^{\\alpha_g}}{x^{\\alpha_g +1}}$ where we want to choose $\\alpha_g$ such that $g(x)$ closely approximates $h(x)f(x)$ in shape. And that is what we want in general, to find such a distribution $g(x)$.\n",
    "\n",
    "With this insight we could change the choice of $g(x)$ in Question 8. To reduce the variance in the integral estimation using importance sampling, you should choose a sampling distribution $g(x)$ that closely matches the shape of the product $h(x)f(x)$. For the integral $\\int_0^1 e^x dx$, an optimal $g(x)$ can be a truncated exponential distribution with parameter $\\lambda$ that approximates the exponential growth of $e^x$ over the interval $[0,1]$. Adjusting $\\lambda$ appropriately will reduce the variance of the importance sampling estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "id": "f68da886-9d3a-459b-8c36-24cf3e0b85b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True mean of the Pareto distribution: 1.6666666666666667\n",
      "IS Estimator for the mean: 1.6714662045013342\n",
      "Variance of the estimator: 2.7417029433120286\n"
     ]
    }
   ],
   "source": [
    "def h(x):\n",
    "    return x\n",
    "    \n",
    "def importance_sampling_pareto(n, alpha, xm, alpha_g):\n",
    "    # Sample from the g sampling distribution with alpha_g\n",
    "    Y = xm * (np.random.uniform(size=n) ** (-1/alpha_g))\n",
    "\n",
    "    f = xm * (Y ** (-1/alpha))\n",
    "    g = xm * (Y ** (-1/alpha_g))\n",
    "    \n",
    "    return np.mean(h(Y) * f/g), np.var(h(Y) * f/g)\n",
    "\n",
    "# Parameters\n",
    "n = 10_000\n",
    "alpha = 2.5\n",
    "xm = 1.0\n",
    "alpha_g = 2.5\n",
    "\n",
    "# Run the importance sampling\n",
    "estimator, variance = importance_sampling_pareto(n, alpha, xm, alpha_g)\n",
    "\n",
    "print(\"True mean of the Pareto distribution:\", alpha * xm / (alpha - 1))\n",
    "print(\"IS Estimator for the mean:\", estimator)\n",
    "print(\"Variance of the estimator:\", variance)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anaconda-panel-2023.05-py310",
   "language": "python",
   "name": "conda-env-anaconda-panel-2023.05-py310-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
